---
title: "yangy22_FinalHomeworkCode_04"
author: "Emily Yang"
date: "`r Sys.Date()`"
output: 
  prettydoc::html_pretty:
    theme: cayman

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Problem 1

Write a simple R function, Z.prop.test(), that can perform one- or two-sample Z-tests for proportion data, using the following guidelines:

Your function should take the following arguments: p1 and n1 (no default) representing the estimated proportion and sample size (i.e., based on your sample data); p2 and n2 (both defaulting to NULL) that contain a second sample’s proportion and sample size data in the event of a two-sample test; p0 (no default) as the expected value for the population proportion; and alternative (default “two.sided”) and conf.level (default 0.95), to be used in the same way as in the function t.test().

Your function should take the following arguments: p1 and n1 (no default) representing the estimated proportion and sample size (i.e., based on your sample data); p2 and n2 (both defaulting to NULL) that contain a second sample’s proportion and sample size data in the event of a two-sample test; p0 (no default) as the expected value for the population proportion; and alternative (default “two.sided”) and conf.level (default 0.95), to be used in the same way as in the function t.test().

When conducting a two-sample test, it should be p1 that is tested as being smaller or larger than p2 when alternative=“less” or alternative=“greater”, the same as in the use of x and y in the function t.test().
The function should perform a one-sample Z-test using p1, n1, and p0 if either p2 or n2 (or both) is NULL.
The function should contain a check for the rules of thumb we have talked about (n∗p>5
 and n∗(1−p)>5
) to ensure the validity of assuming the normal distribution in both the one- and two-sample settings. If this is violated, the function should still complete but it should also print an appropriate warning message.
The function should return a list containing the members Z (the test statistic), P (the appropriate p value), and CI (the two-sided CI with respect to “conf.level” around p1 in the case of a one-sample test and around p2-p1 in the case of a two-sample test). For all test alternatives (“two.sided”, “greater”, “less”), calculate symmetric CIs based on quantiles of the normal distribution rather than worrying about calculating single-limit confidence bounds.
```{r Zprop, eval=FALSE}
library(BSDA)

Z.prop.test <- function(p1, p2=NULL, n1, n2=NULL, p0, alternative = "two.sided", conf.level = 0.95) {   

  # Checking rule of thumb
  if(n1 * p0 < 5 && n1 * (1 - p0) < 5) {print("Warning distribution not normal")}
  
  # One proportion z test
  if (is.null(p2)|| is.null(n2)) {(alternative == "one.sided") 
      z <- (p1 - p0)/sqrt(p0 * (1 - p0) / n1) # Z statistic from Module 10
      sd <-sqrt(p1(1 - p1)/n1) # Calculate SD
      p <- pnorm(z, p1, sd) # P-value from SD
# CI calculations I found from website below
      ciL <- p1 - quantile(z, 1 - 0.12 / 2) * 2 / sqrt(n1) 
      ciU <- p1 + quantile(z, 1 - 0.12 / 2) * 2 / sqrt(n1)
  }
  
  # Two proportion z test
  if (!is.null(p2)|| !is.null(n2) && p1<p2) { (alternative == "lesser") 
       x1 <- p1 * n1
       x2 <- p2 * n2
       pstar <- x1+x2/n1+n2 # Added calculations for pstar from Module 10
       z <- (p2 - p1 - p0)/sqrt(pstar * (1 - pstar) / (1/n1 + 1/n2))
       sd <-sqrt(p1(1 - p1) / n1 + p2(1 - p2) / n2) 
       p <- pnorm(z, p1, sd)
      # Not sure if these CI are correct
       ciL <- p1 - quantile(z, 1 - 0.05 / 2) * 2 / sqrt((n1 + n2/2)) 
       ciU <- p1 + quantile(z, 1 - 0.05 / 2) * 2 / sqrt((n1 + n2/2))
  }  
  
  if  (!is.null(p2)|| !is.null(n2) && p1>p2) { (alternative == "greater") 
       x1 <- p1 * n1
       x2 <- p2 * n2
       pstar <- x1+x2/n1+n2 
       z <- (p2 - p1 - p0) / sqrt(pstar * (1 - pstar) / (1/n1 + 1/n2))
       sd <-sqrt(p1(1-p1)/n1) 
       sd <-sqrt(p1(1-p1)/n1 + p2(1-p2)/n2) 
       ciL <- p1 - quantile(z, 1 - 0.05 / 2) * 2 / sqrt((n1 + n2/2))
       ciU <- p1 + quantile(z, 1 - 0.05 / 2) * 2 / sqrt((n1 + n2/2))
          } 
print(Z=z, Pvalue=p, CILower=ciL, CIUpper=ciU)
}

test <- Z.prop.test(p1=400, n1=100, p0=0.5)
test
```
It tells me: Error in p1(1 - p1) : could not find function "p1". But p1 is specified

## Question 2

Fit the regression model and, using {ggplot2}, produce a scatterplot with the fitted line superimposed upon the data. Append the the fitted model equation to your plot (HINT: use the function geom_text()).

Using your model, add lines for the 90 percent confidence and prediction interval bands on the plot and add a legend to differentiate between the lines
```{r Question 2}
library(curl)
library(ggplot2)
library(ggpubr)
library(tidyverse)

f <- curl("https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/master/AN588_Fall23/KamilarAndCooperData.csv")
d <- read.csv(f, header = TRUE, sep = ",", stringsAsFactors = FALSE)


# I kept these becasue some of my code downstream breaks without it
Brain_Size_Species_Mean <- d$Brain_Size_Species_Mean 
MaxLongevity_m <- d$MaxLongevity_m

# Make the regression
m <- lm(data = d, MaxLongevity_m ~ Brain_Size_Species_Mean)
m
g <- ggplot(data = d, aes(x = MaxLongevity_m, y = Brain_Size_Species_Mean)) 
g <- g +labs(title="Regression of MaxLongevity ~ Brain_Size_Species_Mean",
        x ="MaxLongevity_m", y = "Brain_Size_Species_Mean")
g <- g + geom_point()
g <- g + geom_smooth(method = "lm", formula = y ~ x)
g <- g + annotate("text", x = 200, y = 300, label = "y= 1.218x+248.952") # From lm function
#Lower CI From next chunk
g <- g + geom_abline(intercept = 230.540738, slope = 1.035571, color = "red")
#Upper CI From next chunk
g <- g + geom_abline(intercept = 267.36379, slope = 1.40041, color = "red")
g

# Regression of log version. Repeat of above

n <- lm(data=d, log(MaxLongevity_m) ~ log(Brain_Size_Species_Mean))
n
g <- ggplot(data = d, aes(x = log(MaxLongevity_m), y = log(Brain_Size_Species_Mean)))
g <- g + labs(title="Regression of log(MaxLongevity) ~ log(Brain_Size_Species_Mean)", x ="log(MaxLongevity_m)", y = "log(Brain_Size_Species_Mean)")
g <- g + geom_point()
g <- g + geom_smooth(method = "lm", formula = y ~ x)
g <- g + annotate("text", x = 5, y = 3, label = "y=0.2341x+4.8790") 
g <- g + geom_abline(intercept = 4.7644934, slope = 0.2046396, color = "red")
g <- g + geom_abline(intercept = 4.9934084, slope = 0.2636595, color = "red")
g

```

Identify and interpret the point estimate of the slope (β1), as well as the outcome of the test associated with the hypotheses H0: β1 = 0; HA: β1 ≠ 0. Also, find a 90 percent CI for the slope (β1) parameter.

I used these values to annotate the graph above and add in CI lines but they were way off
```{r CI}
# Calculating beta1 
beta1 <- m$coefficients # Built into lm
beta1 # B1 is 1.21799
beta1 <- n$coefficients # Built into lm
beta1 # B1 is 0.234

#Calculating 90% CI 
ci <- confint(m, level = 0.90)  # Built into lm
ci
ci <- confint(n, level = 0.90)  # Built into lm
ci
```

## Prediction Intervals

I tried only using predict() like in Module 12 but the lines won't stay around my points.
### Prediction interval of MaxLongevity ~ Brain_Size_Species_Mean 
```{r}
#Redoing regression
m <- lm(data = d, MaxLongevity_m ~ Brain_Size_Species_Mean)
h_hat <- predict(m, newdata = data.frame(weight = d$Brain_Size_Species_Mean))
df <- data.frame(cbind(d$Brain_Size_Species_Mean, d$Brain_Size_Species_Mean, h_hat))
names(df) <- c("x", "y", "yhat")

# Predicting confidence intervals
ci <- predict(m, newdata = data.frame(weight = d$Brain_Size_Species_Mean), interval = "confidence", level = 0.90)  # for a vector of values
df <- cbind(df, ci)
names(df) <- c("x", "y", "yhat", "CIfit", "CIlwr", "CIupr")

# Predicting prediction intervals
pi <- predict.lm(m, newdata = data.frame(weight = d$Brain_Size_Species_Mean), interval="prediction", level = 0.90)  
df <- cbind(df, pi)
names(df) <- c("x", "y", "yhat", "CIfit", "CIlwr", "CIupr", "PIfit", "PIlwr",
    "PIupr")

# Plotting everything
g <- ggplot(data = df, aes(x = MaxLongevity_m, y = Brain_Size_Species_Mean))
g <- g +labs(title="Regression of MaxLongevity ~ Brain_Size_Species_Mean",
        x ="MaxLongevity_m", y = "Brain_Size_Species_Mean")
g <- g + geom_point(alpha = 1/2)
g <- g + geom_line(aes(x = x, y = CIfit), colour = "black")
g <- g + geom_line(aes(x = x, y = CIlwr), colour = "blue")
g <- g + geom_line(aes(x = x, y = CIupr), colour = "blue")
g <- g + geom_line(data = df, aes(x = x, y = PIlwr), colour = "red")
g <- g + geom_line(data = df, aes(x = x, y = PIupr), colour = "red")
g
```

### Prediction interval of log(MaxLongevity) ~ log(Brain_Size_Species_Mean

Not sure where my data points went.
```{r}
#Redoing log regression
n <- lm(data=d, log(MaxLongevity_m) ~ log(Brain_Size_Species_Mean))
h_hat <- predict(n, newdata = data.frame(weight = log(d$Brain_Size_Species_Mean)))
df <- data.frame(cbind(log(d$Brain_Size_Species_Mean), log(d$Brain_Size_Species_Mean), h_hat))
names(df) <- c("x", "y", "yhat")

# Predicting confidence intervals
ci <- predict(n, newdata = data.frame(weight = log(d$Brain_Size_Species_Mean)), interval = "confidence", level = 0.90)  # for a vector of values
df <- cbind(df, ci)
names(df) <- c("x", "y", "yhat", "CIfit", "CIlwr", "CIupr")

# Predicting prediction intervals
pi <- predict.lm(n, newdata = data.frame(weight = log(d$Brain_Size_Species_Mean)), interval="prediction", level = 0.90)  
df <- cbind(df, pi)
names(df) <- c("x", "y", "yhat", "CIfit", "CIlwr", "CIupr", "PIfit", "PIlwr",
    "PIupr")

# Plotting everything
g <- ggplot(data = df, aes(x = log(MaxLongevity_m), y = log(Brain_Size_Species_Mean)))
g <- g + labs(title="Regression of log(MaxLongevity) ~ log(Brain_Size_Species_Mean)", x ="log(MaxLongevity_m)", y = "log(Brain_Size_Species_Mean)")
g <- g + geom_line(aes(x = x, y = CIfit), colour = "black")
g <- g + geom_line(aes(x = x, y = CIlwr), colour = "blue")
g <- g + geom_line(aes(x = x, y = CIupr), colour = "blue")
g <- g + geom_line(data = df, aes(x = x, y = PIlwr), colour = "red")
g <- g + geom_line(data = df, aes(x = x, y = PIupr), colour = "red")
g

```

Produce a point estimate and associated 90 percent PI for the longevity of a species whose brain weight is 800 gm. Do you trust the model to predict observations accurately for this value of the explanatory variable? Why or why not?<br>
I would not trust this model to accurately predict this because it is too far out of CI bounds.<br> 

```{r prediction}
# Predicting longevity from brain size
pi1 <- predict(m, newdata = data.frame(Brain_Size_Species_Mean = 800), interval = "prediction", level = 0.90) 
pi1

# # Predicting log longevity from log brain size
pi2 <- predict(n, newdata = data.frame(Brain_Size_Species_Mean = 800), interval = "prediction", level = 0.90)
pi2
```


Looking at your two models, which do you think is better? Why?
<br> 
I think the log model is better because the error is better explained by the line.

##Reflection

This homework was incredibly challenging and I could not complete it even after peer review and office hours. I really gave it my best shot. I think I built the function weirdly because it keeps on telling me that p1 can't be found when I give it p1. I think my issue is that I'm still not fully sure how to use brackets and if statements. I got good feedback about using p0 in my peer review which I incorporated. I have more confidence in my question 2 but I'm not sure why the CI lines I tried to add were so far off. I tried using both confint and predict and the lines look similar but not around my data.<br>  




Sources: 
https://ggplot2.tidyverse.org/reference/annotate.html <br>
https://cran.r-project.org/web/packages/distributions3/vignettes/one-sample-z-confidence-interval.html